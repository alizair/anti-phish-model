{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70704740-3501-49d0-ae76-7121fd005457",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# notes that will save your time \n",
    "to save a model i train\n",
    "import joblib\n",
    "\n",
    "# Saving the model to not retrain later\n",
    "joblib.dump(svm_1, \"svm_1_v1.pkl\")\n",
    "\n",
    "# Load the model later\n",
    "loaded_model = joblib.load(\"svm_1_v1.pkl\")\n",
    "\n",
    "# Make predictions with the loaded model\n",
    "y_pred = loaded_model.predict(X1_test)\n",
    "# to save tfidf vectorizations:\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "sparse.save_npz(\"./vectorizations/tfidf_v1/X1_train_tfidf.npz\", X1_train)\n",
    "sparse.save_npz(\"./vectorizations/tfidf_v1/X1_test_tfidf.npz\", X1_test)\n",
    "sparse.save_npz(\"./vectorizations/tfidf_v1/X1_val_tfidf.npz\", X1_val)\n",
    "\n",
    "# --- Save labels as numpy arrays ---\n",
    "np.save(\"./vectorizations/tfidf_v1/y1_train_tfidf.npy\", y1_train.to_numpy())\n",
    "np.save(\"./vectorizations/tfidf_v1/y1_test_tfidf.npy\", y1_test.to_numpy())\n",
    "np.save(\"./vectorizations/tfidf_v1/y1_val_tfidf.npy\", y1_val.to_numpy())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d934d-cc98-4a91-b943-58386e55ee0f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#personal issue with cuda..leave for me later\n",
    "#i ran these commands:\n",
    "#pip uninstall torch torchvision torchaudio -y -v\n",
    "#pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128 -v\n",
    "#took some time and it worked, make sure the cuda version matches your gpu\n",
    "import torch\n",
    "# Verify CUDA is available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"GPU count:\", torch.cuda.device_count())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad5f771-03c5-467c-ad2d-98bb5d567e4c",
   "metadata": {},
   "source": [
    "**FIRST: Importing The Dataset**\n",
    "\n",
    "We plan to use the exact same training, testing and validation elements. so we will import them from the same exact csv files to ensure fair and accurate comparison between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63df5730-69a5-4686-b573-975328d846d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (126035, 4)\n",
      "Test shape: (42012, 4)\n",
      "Validation shape: (42012, 4)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"../split_dataset/train_split.csv\")\n",
    "test_df = pd.read_csv(\"../split_dataset/test_split.csv\")\n",
    "val_df = pd.read_csv(\"../split_dataset/val_split.csv\")\n",
    "\n",
    "# Quick checks\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa60d8-e66f-4e26-b61d-fc5d4f63deef",
   "metadata": {},
   "source": [
    "**Support Vector Machines (SVM)** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55980a-deac-4791-aee0-47ac25adf7dc",
   "metadata": {},
   "source": [
    "**Preprocessing Method #1: TF-IDF v1**\n",
    "\n",
    "TF-IDF is one of the classic vectorization techniques when it comes to text. \n",
    "It is simple and fast, computationally cheaper, interpretable; as in you can see which words have the highest weight for a document.It can also reduce the impact of common words.\n",
    "some cons to using TF-IDF: it ignores word order and context, which can heavily affect accuracy when it comes to predicting phishing/ spam emails. it can be sparse and high dimensional, for example, if we have 200k unique words, each email is a 200k dimensional sparse vector. which can be crazy to use for training, and storing in a databse.\n",
    "\n",
    "\n",
    "`max_features` controls the vocabulary size, if it's too small it can lead to losing important information on some words, if it's too large we have the risk of overfitting and longer training time, with more memory consumption.\n",
    "for `max_features`, a good balanced number is usually around 5k - 20k, therefore we will try to train using different numbers in this range. \n",
    "\n",
    "`stop_words` are common words like \"the\", \"is\", \"of\"...etc. They don't add much meaning in the classification process. having it can help with training accuracy by making the model focus on other, possibly more important words.\n",
    "but there may be a risk where it can hurt the performance, therefore we will have a run not including it. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20a376-e0ee-42bd-8260-08fc9313cd42",
   "metadata": {},
   "source": [
    "**TF-IDF_v1**\n",
    "\n",
    "For this run, we are using `stop_words = english` to remove the common mostly useless repetative words in the english language as explained before.\n",
    "We are setting `max_features = 10000` for subject, since subjects usually tend to have less words than the body and we wish to only consider important words used in a phishing context. while for body we will take `max_features = 15000` as a start. \n",
    "\n",
    "*Notice: after we see the shape, it says 25000 columns, since we combined the subject and body vectors for this run.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaaf42ac-a90f-47e0-b91a-52e5fe2f16fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 (TF-IDF separate subject+body) shapes:\n",
      "Train: (126035, 25000)  Test: (42012, 25000)  Val: (42012, 25000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "\n",
    "# --- TF-IDF Run #1  ---\n",
    "vectorizer1_subject = TfidfVectorizer(\n",
    "    max_features=10000,   \n",
    "    stop_words=\"english\"\n",
    ")\n",
    "vectorizer1_body = TfidfVectorizer(\n",
    "    max_features=15000,  \n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X1_train_subject = vectorizer1_subject.fit_transform(train_df[\"subject\"].fillna(\"\"))\n",
    "X1_test_subject = vectorizer1_subject.transform(test_df[\"subject\"].fillna(\"\"))\n",
    "X1_val_subject = vectorizer1_subject.transform(val_df[\"subject\"].fillna(\"\"))\n",
    "\n",
    "X1_train_body = vectorizer1_body.fit_transform(train_df[\"body\"].fillna(\"\"))\n",
    "X1_test_body = vectorizer1_body.transform(test_df[\"body\"].fillna(\"\"))\n",
    "X1_val_body = vectorizer1_body.transform(val_df[\"body\"].fillna(\"\"))\n",
    "\n",
    "# we combine subject and body vectors using hstack\n",
    "X1_train = hstack([X1_train_subject, X1_train_body])\n",
    "X1_test = hstack([X1_test_subject, X1_test_body])\n",
    "X1_val = hstack([X1_val_subject, X1_val_body])\n",
    "\n",
    "\n",
    "y1_train = train_df[\"isPhishing\"]\n",
    "y1_test = test_df[\"isPhishing\"]\n",
    "y1_val = val_df[\"isPhishing\"]\n",
    "\n",
    "print(\"Run 1 (TF-IDF separate subject+body) shapes:\")\n",
    "print(\"Train:\", X1_train.shape, \" Test:\", X1_test.shape, \" Val:\", X1_val.shape)\n",
    "\n",
    "\n",
    "\n",
    "sparse.save_npz(\"./vectorizations/tfidf_v1/X1_train_tfidf.npz\", X1_train)\n",
    "sparse.save_npz(\"./vectorizations/tfidf_v1/X1_test_tfidf.npz\", X1_test)\n",
    "sparse.save_npz(\"./vectorizations/tfidf_v1/X1_val_tfidf.npz\", X1_val)\n",
    "\n",
    "# --- Save labels as numpy arrays ---\n",
    "np.save(\"./vectorizations/tfidf_v1/y1_train_tfidf.npy\", y1_train.to_numpy())\n",
    "np.save(\"./vectorizations/tfidf_v1/y1_test_tfidf.npy\", y1_test.to_numpy())\n",
    "np.save(\"./vectorizations/tfidf_v1/y1_val_tfidf.npy\", y1_val.to_numpy())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0ab6c-162f-491f-9d3e-72fa3ef6681d",
   "metadata": {},
   "source": [
    "**Preprocessing Method #1: TF-IDF v2**\n",
    "\n",
    "[Combine subject and body to possibly reduce overfitting caused by having duplicate of key words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7e2773b-06ad-4ae7-b01d-94084a31edec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF combined subject+body shapes:\n",
      "Train: (126035, 20000)  Test: (42012, 20000)  Val: (42012, 20000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# --- Combine subject + body into a single column ---\n",
    "def combine_texts(df):\n",
    "    return (df[\"subject\"].fillna(\"\") + \" \" + df[\"body\"].fillna(\"\")).str.strip()\n",
    "\n",
    "train_texts = combine_texts(train_df)\n",
    "test_texts = combine_texts(test_df)\n",
    "val_texts = combine_texts(val_df)\n",
    "\n",
    "# --- TF-IDF Vectorization ---\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,  # combined, so sum of previous max_features\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "X_val = vectorizer.transform(val_texts)\n",
    "\n",
    "y_train = train_df[\"isPhishing\"]\n",
    "y_test = test_df[\"isPhishing\"]\n",
    "y_val = val_df[\"isPhishing\"]\n",
    "\n",
    "print(\"TF-IDF combined subject+body shapes:\")\n",
    "print(\"Train:\", X_train.shape, \" Test:\", X_test.shape, \" Val:\", X_val.shape)\n",
    "\n",
    "# --- Save embeddings ---\n",
    "sparse.save_npz(\"./vectorizations/tfidf_v2/X_train_tfidf.npz\", X_train)\n",
    "sparse.save_npz(\"./vectorizations/tfidf_v2/X_test_tfidf.npz\", X_test)\n",
    "sparse.save_npz(\"./vectorizations/tfidf_v2/X_val_tfidf.npz\", X_val)\n",
    "\n",
    "# --- Save labels ---\n",
    "np.save(\"./vectorizations/tfidf_v2/y_train_tfidf.npy\", y_train.to_numpy())\n",
    "np.save(\"./vectorizations/tfidf_v2/y_test_tfidf.npy\", y_test.to_numpy())\n",
    "np.save(\"./vectorizations/tfidf_v2/y_val_tfidf.npy\", y_val.to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce1c43c-2283-4f88-bf2d-a06ab06ac8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a4c5a4c-711b-4562-9799-91e0c481da63",
   "metadata": {},
   "source": [
    "**Preprocessing Method #2: SBERT v1**\n",
    "\n",
    "[Describe SBIRT]\n",
    "[Why you plan to use it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37f1f80e-b761-4301-8292-4dd1b5f6a47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train texts with NaN: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8d601a124a44609c174b0874baea35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c2392a24694b15b84e35e2be248d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93929cfaa3664c17918cebe290e5c0ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: (126035, 768)\n",
      "Test embeddings shape: (42012, 768)\n",
      "Validation embeddings shape: (42012, 768)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Combine subject + body and handle NaN values\n",
    "def combine_texts(df):\n",
    "    # Fill NaN with empty string first\n",
    "    subject = df[\"subject\"].fillna(\"\")\n",
    "    body = df[\"body\"].fillna(\"\")\n",
    "    # Combine\n",
    "    texts = subject + \" \" + body\n",
    "    # Strip whitespace and replace empty strings with a placeholder\n",
    "    texts = texts.str.strip()\n",
    "    texts = texts.replace(\"\", \"empty\")  # or use \"no content\"\n",
    "    return texts\n",
    "\n",
    "train_texts = combine_texts(train_df)\n",
    "test_texts = combine_texts(test_df)\n",
    "val_texts = combine_texts(val_df)\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"Train texts with NaN: {train_texts.isna().sum()}\")\n",
    "#print(f\"Sample train text: {train_texts.iloc[0]}\")\n",
    "\n",
    "# Encode - REMOVE the device parameter\n",
    "X_train_sbert = model.encode(train_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "X_test_sbert  = model.encode(test_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "X_val_sbert   = model.encode(val_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# Labels\n",
    "y_train_sbert = train_df[\"isPhishing\"]\n",
    "y_test_sbert  = test_df[\"isPhishing\"]\n",
    "y_val_sbert   = val_df[\"isPhishing\"]\n",
    "\n",
    "print(\"Train embeddings shape:\", X_train_sbert.shape)\n",
    "print(\"Test embeddings shape:\", X_test_sbert.shape)\n",
    "print(\"Validation embeddings shape:\", X_val_sbert.shape)\n",
    "\n",
    "# Save embeddings\n",
    "np.save('./vectorizations/sbert_v1/X_train_sbert.npy', X_train_sbert)\n",
    "np.save('./vectorizations/sbert_v1/X_test_sbert.npy', X_test_sbert)\n",
    "np.save('./vectorizations/sbert_v1/X_val_sbert.npy', X_val_sbert)\n",
    "\n",
    "# Save labels\n",
    "np.save('./vectorizations/sbert_v1/y_train_sbert.npy', y_train_sbert.to_numpy())\n",
    "np.save('./vectorizations/sbert_v1/y_test_sbert.npy', y_test_sbert.to_numpy())\n",
    "np.save('./vectorizations/sbert_v1/y_val_sbert.npy', y_val_sbert.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb08613-a087-443a-998d-6da1a53b18b3",
   "metadata": {},
   "source": [
    "**Preprocessing Method #2: SBERT v2**\n",
    "\n",
    "[talk about the lighter DL model you will use for this run: all-MiniLM-L6-v2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac29ba5-3ba9-4d4b-8a12-9c847b303808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train texts with NaN: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d553360828334fd9b5e13b643e396936",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6adff995032493f8917af72e463a031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0984d3da3cf4fb5a1edf6ade9ea7456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: (126035, 384)\n",
      "Test embeddings shape: (42012, 384)\n",
      "Validation embeddings shape: (42012, 384)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Combine subject + body and handle NaN values\n",
    "def combine_texts(df):\n",
    "    # Fill NaN with empty string first\n",
    "    subject = df[\"subject\"].fillna(\"\")\n",
    "    body = df[\"body\"].fillna(\"\")\n",
    "    # Combine\n",
    "    texts = subject + \" \" + body\n",
    "    # Strip whitespace and replace empty strings with a placeholder\n",
    "    texts = texts.str.strip()\n",
    "    texts = texts.replace(\"\", \"empty\")  # or use \"no content\"\n",
    "    return texts\n",
    "\n",
    "train_texts = combine_texts(train_df)\n",
    "test_texts = combine_texts(test_df)\n",
    "val_texts = combine_texts(val_df)\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"Train texts with NaN: {train_texts.isna().sum()}\")\n",
    "#print(f\"Sample train text: {train_texts.iloc[0]}\")\n",
    "\n",
    "# Encode - REMOVE the device parameter\n",
    "X_train_sbert = model.encode(train_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "X_test_sbert  = model.encode(test_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "X_val_sbert   = model.encode(val_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# Labels\n",
    "y_train_sbert = train_df[\"isPhishing\"]\n",
    "y_test_sbert  = test_df[\"isPhishing\"]\n",
    "y_val_sbert   = val_df[\"isPhishing\"]\n",
    "\n",
    "print(\"Train embeddings shape:\", X_train_sbert.shape)\n",
    "print(\"Test embeddings shape:\", X_test_sbert.shape)\n",
    "print(\"Validation embeddings shape:\", X_val_sbert.shape)\n",
    "\n",
    "# Save embeddings\n",
    "np.save('./vectorizations/sbert_v2/X_train_sbert.npy', X_train_sbert)\n",
    "np.save('./vectorizations/sbert_v2/X_test_sbert.npy', X_test_sbert)\n",
    "np.save('./vectorizations/sbert_v2/X_val_sbert.npy', X_val_sbert)\n",
    "\n",
    "# Save labels\n",
    "np.save('./vectorizations/sbert_v2/y_train_sbert.npy', y_train_sbert.to_numpy())\n",
    "np.save('./vectorizations/sbert_v2/y_test_sbert.npy', y_test_sbert.to_numpy())\n",
    "np.save('./vectorizations/sbert_v2/y_val_sbert.npy', y_val_sbert.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd78fa0-639c-45b0-91ab-fd09a7a8d591",
   "metadata": {},
   "source": [
    "**Preprocessing Method #2: SBERT v3**\n",
    "\n",
    "[[paraphrase-distilroberta-base-v2] trying this model of sbert]\n",
    "\n",
    "[it is not as accurate as the first, but still faster. (paraphrase-mpnet-base-v2)]\n",
    "\n",
    "[more accurate than (all-MiniLM-L6-v2), and slightly slower]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d05b6d3f-78db-469b-934a-14e6bf713d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train texts with NaN: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a83bfa230142e39340360bb40ec473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "664efca5548a46ba88ac3d0c4153c20c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d2f50701a5341e989a77edfff356e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: (126035, 768)\n",
      "Test embeddings shape: (42012, 768)\n",
      "Validation embeddings shape: (42012, 768)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v2')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Combine subject + body and handle NaN values\n",
    "def combine_texts(df):\n",
    "    # Fill NaN with empty string first\n",
    "    subject = df[\"subject\"].fillna(\"\")\n",
    "    body = df[\"body\"].fillna(\"\")\n",
    "    # Combine\n",
    "    texts = subject + \" \" + body\n",
    "    # Strip whitespace and replace empty strings with a placeholder\n",
    "    texts = texts.str.strip()\n",
    "    texts = texts.replace(\"\", \"empty\")  # or use \"no content\"\n",
    "    return texts\n",
    "\n",
    "train_texts = combine_texts(train_df)\n",
    "test_texts = combine_texts(test_df)\n",
    "val_texts = combine_texts(val_df)\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"Train texts with NaN: {train_texts.isna().sum()}\")\n",
    "#print(f\"Sample train text: {train_texts.iloc[0]}\")\n",
    "\n",
    "# Encode - REMOVE the device parameter\n",
    "X_train_sbert = model.encode(train_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "X_test_sbert  = model.encode(test_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "X_val_sbert   = model.encode(val_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# Labels\n",
    "y_train_sbert = train_df[\"isPhishing\"]\n",
    "y_test_sbert  = test_df[\"isPhishing\"]\n",
    "y_val_sbert   = val_df[\"isPhishing\"]\n",
    "\n",
    "print(\"Train embeddings shape:\", X_train_sbert.shape)\n",
    "print(\"Test embeddings shape:\", X_test_sbert.shape)\n",
    "print(\"Validation embeddings shape:\", X_val_sbert.shape)\n",
    "\n",
    "# Save embeddings\n",
    "np.save('./vectorizations/sbert_v3/X_train_sbert.npy', X_train_sbert)\n",
    "np.save('./vectorizations/sbert_v3/X_test_sbert.npy', X_test_sbert)\n",
    "np.save('./vectorizations/sbert_v3/X_val_sbert.npy', X_val_sbert)\n",
    "\n",
    "# Save labels\n",
    "np.save('./vectorizations/sbert_v3/y_train_sbert.npy', y_train_sbert.to_numpy())\n",
    "np.save('./vectorizations/sbert_v3/y_test_sbert.npy', y_test_sbert.to_numpy())\n",
    "np.save('./vectorizations/sbert_v3/y_val_sbert.npy', y_val_sbert.to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced0c4d-8c4e-4118-93c5-dca3ac2063da",
   "metadata": {},
   "source": [
    "**Preprocessing Method #2: SBERT v4*\n",
    "*\n",
    "[Check the multilingual sbert model]\n",
    "\n",
    "distiluse-base-multilingual-cased-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e27c55fb-cc36-492a-ac2a-2defd013b710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Train texts with NaN: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9a1cf2fdcc843c9b46e2e5dab67b9a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1970 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7855a25cf09740339edf5538ebe1e6c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8a99b2e83a47688cb7c2b02da9e8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/657 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train embeddings shape: (126035, 512)\n",
      "Test embeddings shape: (42012, 512)\n",
      "Validation embeddings shape: (42012, 512)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Combine subject + body and handle NaN values\n",
    "def combine_texts(df):\n",
    "    # Fill NaN with empty string first\n",
    "    subject = df[\"subject\"].fillna(\"\")\n",
    "    body = df[\"body\"].fillna(\"\")\n",
    "    # Combine\n",
    "    texts = subject + \" \" + body\n",
    "    # Strip whitespace and replace empty strings with a placeholder\n",
    "    texts = texts.str.strip()\n",
    "    texts = texts.replace(\"\", \"empty\")  # or use \"no content\"\n",
    "    return texts\n",
    "\n",
    "train_texts = combine_texts(train_df)\n",
    "test_texts = combine_texts(test_df)\n",
    "val_texts = combine_texts(val_df)\n",
    "\n",
    "# Check for any remaining issues\n",
    "print(f\"Train texts with NaN: {train_texts.isna().sum()}\")\n",
    "#print(f\"Sample train text: {train_texts.iloc[0]}\")\n",
    "\n",
    "# Encode - REMOVE the device parameter\n",
    "X_train_sbert = model.encode(train_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "X_test_sbert  = model.encode(test_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "X_val_sbert   = model.encode(val_texts.tolist(), batch_size=64, show_progress_bar=True)\n",
    "\n",
    "# Labels\n",
    "y_train_sbert = train_df[\"isPhishing\"]\n",
    "y_test_sbert  = test_df[\"isPhishing\"]\n",
    "y_val_sbert   = val_df[\"isPhishing\"]\n",
    "\n",
    "print(\"Train embeddings shape:\", X_train_sbert.shape)\n",
    "print(\"Test embeddings shape:\", X_test_sbert.shape)\n",
    "print(\"Validation embeddings shape:\", X_val_sbert.shape)\n",
    "\n",
    "# Save embeddings\n",
    "np.save('./vectorizations/sbert_v4/X_train_sbert.npy', X_train_sbert)\n",
    "np.save('./vectorizations/sbert_v4/X_test_sbert.npy', X_test_sbert)\n",
    "np.save('./vectorizations/sbert_v4/X_val_sbert.npy', X_val_sbert)\n",
    "\n",
    "# Save labels\n",
    "np.save('./vectorizations/sbert_v4/y_train_sbert.npy', y_train_sbert.to_numpy())\n",
    "np.save('./vectorizations/sbert_v4/y_test_sbert.npy', y_test_sbert.to_numpy())\n",
    "np.save('./vectorizations/sbert_v4/y_val_sbert.npy', y_val_sbert.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a685a-d3e5-4836-ae15-b4649625cf64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
