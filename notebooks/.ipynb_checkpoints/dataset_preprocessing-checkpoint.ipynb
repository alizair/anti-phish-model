{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "48afa18d-7de3-4229-befc-eae575d69854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omark\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Model Building\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#--------------------------------------------\n",
    "\n",
    "#Goal: create the final dataset from the 7 email phishing dataset (the union of all small datasets)\n",
    "# list of current baby sets: data1, data2, data3, data4, data5, data6, data7\n",
    "# to do list:\n",
    "# 1. try to reach a good number of emails, around 100k        [  X  ] \n",
    "# 2. figure out what to do with the Nan datasets: data2_nan, data3_nan,data4_nan,data5_nan  [  ]\n",
    "# 3. deal witht he weird ascii data in all datasets, best to remove them imo, but for later [  ]\n",
    "# 4. before joining the small datasets, make sure to remove sender from dataset 1, 2 , 3 , 4, 6. and add url (1 if contains, 0 if not) to dataset 5, 7. [ X ] \n",
    "# statistics of final_dataset:\n",
    "#(202917, 4), 94305 is phishing, 108612 is safe\n",
    "\n",
    "url_pattern = r'(https?://\\S+|www\\.\\S+)'\n",
    "#--------------------------------------------\n",
    "data1 = pd.read_csv(\"../datasets/CEAS_08.csv\",encoding='latin1')\n",
    "#CEAS 8 Dataset, has after removing the unwanted columns (39154, 5),   21842 is phishing, 17312 is safe , dtype of label is int64\n",
    "\n",
    "#preprocessing so far for it ( no vectorization, just data cleaning...etc)\n",
    "data1.drop(columns=[\"sender\",\"receiver\", \"date\"], inplace=True)\n",
    "data1.rename(columns={'label': 'isPhishing'}, inplace=True) \n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "data2 = pd.read_csv(\"../datasets/TREC_07.csv\", encoding='latin1', engine='python', on_bad_lines='skip')\n",
    "#TREC 7 Dataset, has (53745, 5),   29392 is phishing, 24353   is safe , dtype of label is now int64, used to be object with other out of place labels( None, '   ', 'const')\n",
    "data2.drop(columns=[\"sender\",\"receiver\", \"date\"], inplace=True)\n",
    "data2.rename(columns={'label': 'isPhishing'}, inplace=True) \n",
    "\n",
    "#could probably do this step in a ..cleaner more effecient way...removing any value other than 1 or 0\n",
    "data2_placeholder= data2[data2['isPhishing'].isin(['0', '1'])]\n",
    "data2 = data2_placeholder\n",
    "del data2_placeholder\n",
    "\n",
    "#i removed the rows with label Nan in \"isPhishing\", will see what to do with that later, saved the Nan rows to data2_nan\n",
    "data2_nan = data2[data2['isPhishing'].isna()]\n",
    "data2 = data2.dropna(subset=['isPhishing'])\n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "data3 = pd.read_csv(\"../datasets/TREC_06.csv\", encoding='latin1',index_col=0, engine='python', on_bad_lines='skip') # i added teh index col because it fixed a problem..god bless stackoverflow\n",
    "\n",
    "# Trec 6, has (16382, 5), 3989 are phsihing, 12393 are safe, dtype of label is int 64 as well, make sure numbers are updated after cleaning\n",
    "#NAn problem handled first, decide to kill it or keep it later\n",
    "data3_nan = data3[data3['label'].isna()]\n",
    "data3 = data3.dropna(subset=['label'])\n",
    "data3.drop(columns=[\"receiver\", \"date\"], inplace=True)\n",
    "data3.rename(columns={'label': 'isPhishing'}, inplace=True) \n",
    "\n",
    "#--------------------------------------------\n",
    "\n",
    "data4 = pd.read_csv(\"../datasets/TREC_05.csv\", encoding='latin1', engine='python', on_bad_lines='skip')\n",
    "#Trec 5, has (55210, 5), 22932 are phishing, 22932 are safe\n",
    "#Nan Problem\n",
    "data4_nan = data4[data4['label'].isna()]\n",
    "data4 = data4.dropna(subset=['label'])\n",
    "#Label mismatch type problem \n",
    "data4 = data4[data4['label'].isin(['0', '1'])]\n",
    "data4['label'] = data4['label'].astype(int)\n",
    "\n",
    "#dropping 2 columns and renaming one\n",
    "data4.drop(columns=[\"sender\",\"receiver\", \"date\"], inplace=True)\n",
    "data4.rename(columns={'label': 'isPhishing'}, inplace=True) \n",
    "\n",
    "#--------------------------------------------\n",
    "data5 = pd.read_csv(\"../datasets/Enron.csv\", encoding='latin1', engine='python', on_bad_lines='skip')\n",
    "#features: subject, body, isphishing\n",
    "#Enron , has (29763, 3), 13976 are phishing, 15787 safe, Enron has weird emails probably better to discard it!!!!!!!!!!!!!!!!!!!!\n",
    "data5.rename(columns={'label': 'isPhishing'}, inplace=True)\n",
    "data5_nan = data5[data5['isPhishing'].isna()]\n",
    "data5 = data5.dropna(subset=['isPhishing'])\n",
    "data5 = data5[data5['isPhishing'].isin(['0', '1'])]\n",
    "data5['isPhishing'] = data5['isPhishing'].astype('int64')\n",
    "#add urls column\n",
    "data5['urls'] = 0   # creates a new column 'url' and fills it with 0\n",
    "data5['urls'] = data5['body'].apply(lambda x: 1 if re.search(url_pattern, str(x)) else 0)\n",
    "\n",
    "#--------------------------------------------\n",
    "data6 = pd.read_csv(\"../datasets/Assassin.csv\", encoding='latin1', engine='python', on_bad_lines='skip')\n",
    "#Assassin dataset, (5805, 5), 1716 phishing, 4089 safe, \n",
    "data6.drop(columns=[\"sender\",\"receiver\", \"date\"], inplace=True)\n",
    "data6.rename(columns={'label': 'isPhishing'}, inplace=True) \n",
    "data6_nan = data6[data6['isPhishing'].isna()]\n",
    "data6 = data6.dropna(subset=['isPhishing'])\n",
    "data6['isPhishing'] = data6['isPhishing'].round().astype('int64')\n",
    "\n",
    "#--------------------------------------------\n",
    "data7 = pd.read_csv(\"../datasets/Ling.csv\", encoding='latin1', engine='python', on_bad_lines='skip')\n",
    "# Ling, has 3 features subject, body, isPhishing,(2859, 3), 458 phishing emails, 2401 safe emails.\n",
    "data7.rename(columns={'label': 'isPhishing'}, inplace=True) \n",
    "data7_nan = data7[data7['isPhishing'].isna()]\n",
    "data7 = data7.dropna(subset=['isPhishing'])\n",
    "#add urls column\n",
    "data7['urls'] = 0   # creates a new column 'url' and fills it with 0\n",
    "data7['urls'] = data5['body'].apply(lambda x: 1 if re.search(url_pattern, str(x)) else 0)\n",
    "\n",
    "#Combine all datasets\n",
    "final_data = pd.concat([data1, data2, data3, data4, data5, data6, data7], ignore_index=True)\n",
    "final_data['isPhishing'] = final_data['isPhishing'].astype(int)\n",
    "final_data['urls'] = final_data['isPhishing'].astype(int)\n",
    "# 2.Drop exact duplicates\n",
    "final_data = final_data.drop_duplicates()\n",
    "final_data = final_data.reset_index(drop=True)\n",
    "# save it to a csv\n",
    "final_data.to_csv('../datasets/final_dataset.csv', index=False, encoding='latin1', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20a86f5-c27f-4e96-a6fd-ced6c9d6e27e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# some useful functions!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#to check a specific row\n",
    "data1.loc[21609, 'body']\n",
    "#check a sample\n",
    "data1.sample(15)\n",
    "#check features/ columns\n",
    "data1.columns\n",
    "#check general overview (rows and number of columns)\n",
    "data1.shape\n",
    "#see how many rows are in the dataset\n",
    "len(data1)\n",
    "\n",
    "# delete a dataset\n",
    "del data223344\n",
    "#drop a column\n",
    "data4.drop(columns=[\"c1\", \"c2\"], inplace=True)\n",
    "# rename a column\n",
    "data4.rename(columns={'oldname': 'newname'}, inplace=True) \n",
    "#check type and number of unique values\n",
    "print(data1['isPhishing'].dtype)     \n",
    "print(data1['isPhishing'].unique())   # check the unique values returns an array\n",
    "# check the number of rows with each value\n",
    "data1['label'].value_counts()\n",
    "data3['label'].value_counts(dropna=False)\n",
    "\n",
    "#convert to another datatype\n",
    "data1['label'] = data['label'].astype(new data type)\n",
    "#keep certain values:\n",
    "# Keep only rows where label is 0 or 1\n",
    "data3 = data3[data3['label'].isin(['0', '1'])]\n",
    "#look at one row\n",
    "data1.loc[123,'column']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627fd3b-2277-40ee-a0b3-755658a05b01",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Plotting \n",
    "# Set style for better looking plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create a figure with multiple subplots\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "\n",
    "# 1. Basic Distribution of Phishing vs Non-Phishing\n",
    "plt.subplot(2, 3, 1)\n",
    "counts = final_data['isPhishing'].value_counts()\n",
    "labels = ['Non-Phishing (0)', 'Phishing (1)']\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "plt.pie(counts.values, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "plt.title('Distribution of Phishing vs Non-Phishing Emails', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 2. Bar plot of the same data\n",
    "plt.subplot(2, 3, 2)\n",
    "bars = plt.bar(['Non-Phishing', 'Phishing'], counts.values, color=colors)\n",
    "plt.title('Email Count by Category', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Emails')\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{int(height):,}',\n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. If you have email length data (create if you don't have it)\n",
    "if 'email_length' not in final_data.columns:\n",
    "    # Assuming you have a text column, replace 'email_text' with your actual column name\n",
    "    # final_data['email_length'] = final_data['email_text'].str.len()\n",
    "    # For demonstration, let's create synthetic length data\n",
    "    np.random.seed(42)\n",
    "    final_data['email_length'] = np.random.normal(500, 200, len(final_data))\n",
    "    final_data['email_length'] = np.abs(final_data['email_length'])  # Make sure no negative lengths\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "phishing_lengths = final_data[final_data['isPhishing'] == 1]['email_length']\n",
    "non_phishing_lengths = final_data[final_data['isPhishing'] == 0]['email_length']\n",
    "\n",
    "plt.hist([non_phishing_lengths, phishing_lengths], bins=50, alpha=0.7, \n",
    "         label=['Non-Phishing', 'Phishing'], color=colors)\n",
    "plt.title('Email Length Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Email Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# 4. Box plot comparing email lengths\n",
    "plt.subplot(2, 3, 4)\n",
    "data_to_plot = [non_phishing_lengths, phishing_lengths]\n",
    "box_plot = plt.boxplot(data_to_plot, labels=['Non-Phishing', 'Phishing'], patch_artist=True)\n",
    "box_plot['boxes'][0].set_facecolor(colors[0])\n",
    "box_plot['boxes'][1].set_facecolor(colors[1])\n",
    "plt.title('Email Length Comparison', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Email Length (characters)')\n",
    "\n",
    "\n",
    "# 5. Summary statistics\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.axis('off')  # Turn off axis for text display\n",
    "\n",
    "# Calculate some statistics\n",
    "total_emails = len(final_data)\n",
    "phishing_count = (final_data['isPhishing'] == 1).sum()\n",
    "non_phishing_count = (final_data['isPhishing'] == 0).sum()\n",
    "phishing_percentage = (phishing_count / total_emails) * 100\n",
    "\n",
    "stats_text = f\"\"\"\n",
    "Dataset Summary Statistics\n",
    "\n",
    "Total Emails: {total_emails:,}\n",
    "\n",
    "Phishing Emails: {phishing_count:,}\n",
    "Non-Phishing Emails: {non_phishing_count:,}\n",
    "\n",
    "Phishing Rate: {phishing_percentage:.1f}%\n",
    "\n",
    "Dataset Balance:\n",
    "{'Balanced' if abs(phishing_percentage - 50) < 10 else 'Imbalanced'}\n",
    "\n",
    "Avg Email Length:\n",
    "Non-Phishing: {non_phishing_lengths.mean():.0f} chars\n",
    "Phishing: {phishing_lengths.mean():.0f} chars\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.1, 0.5, stats_text, fontsize=11, verticalalignment='center',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.7))\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Phishing Email Dataset Analysis', fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.subplots_adjust(top=0.93)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Individual plots with more detail\n",
    "def create_detailed_distribution_plot():\n",
    "    \"\"\"Create a more detailed distribution plot\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create a more detailed bar plot\n",
    "    counts = final_data['isPhishing'].value_counts().sort_index()\n",
    "    bars = plt.bar(['Non-Phishing (0)', 'Phishing (1)'], counts.values, \n",
    "                   color=['#3498db', '#e74c3c'], alpha=0.8, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    plt.title('Phishing Email Dataset Distribution', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.ylabel('Number of Emails', fontsize=12)\n",
    "    plt.xlabel('Email Category', fontsize=12)\n",
    "    \n",
    "    # Add value labels and percentages\n",
    "    total = counts.sum()\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        percentage = (height / total) * 100\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + total*0.01,\n",
    "                f'{int(height):,}\\n({percentage:.1f}%)',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # Add grid for better readability\n",
    "    plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    plt.ylim(0, max(counts.values) * 1.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the detailed plot function\n",
    "create_detailed_distribution_plot()\n",
    "\n",
    "print(\"Plots generated successfully!\")\n",
    "print(f\"Dataset contains {len(final_data):,} emails\")\n",
    "print(f\"Phishing emails: {(final_data['isPhishing'] == 1).sum():,}\")\n",
    "print(f\"Non-phishing emails: {(final_data['isPhishing'] == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88126d2e-66d3-4d9d-b4a4-71e178c50d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202917, 4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f75f771e-ed4d-49cd-a961-525cabbb1798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "urls\n",
       "0    108612\n",
       "1     94305\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['urls'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97899edc-4bf3-4185-b3a9-4900e67e8d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
